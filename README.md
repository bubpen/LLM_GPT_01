# 트랜스포머 모델(Transformer model)이란?
트랜스포머 모델은 자연어 처리(NLP)와 딥러닝에서 혁신을 일으킨 모델로, 구글이 2017년 논문에서 처음 제안했다. 이 모델은 Seq2Seq(Sequence-to-Sequence) 구조를 기반으로 하며, RNN이나 LSTM 같은 순차적 처리 방식 대신 Attention 매커니즘을 중심으로 설계된 것이 특징이다.
### 트랜스포머의 주요 구성 요소
<b>1. 인코더(Encoder)와 디코더(Decoder)</b>
  - 트랜스포머는 인코더와 디코더라는 두 블록으로 구성돼있다.
    - <b>인코더 : </b>입력 문장을 처리해 중요한 정보를 추출.
    - <b>디코더 : </b>인코더가 생생한 정보를 바탕으로 출력을 생성.
  - 번역같은 작업에서 입력을 받아 출력을 생성하는 방식으로 동작
    - 예) 입력 : 영어 -> 출력 : 한국어
<b>2. Attention 매커니즘</b>
  - 트랜스포머는 핵심적으로 <b>Self-Attention</b>과 <b>Cross-Attention</b>을 사용.
    - <b>Self-Attention : </b>각 단어가 문장의 다른 모든 단어와의 관계를 이해하도록 함.
    - <b>Cross-Attetion : </b>디코더와 인코더의 출력에서 중요한 정보를 선택.
  - 이 덕분에 문맥 정보를 매우 잘 이해할 수 있다.
<b>3. 멀티헤드 어텐션(Multi-Head Attention)</b>
  -여러 개의 Attention Head를 병렬적으로 활용해, 다양한 문맥 정보를 동시에 학습.
  - 각 Head는 서로 다른 패턴에 집중하기 때문에 더 풍부한 표현 학습이 가능.
<b>4. 포지셔널 인코딩(Positional Encoding)</b>
  - Transformer는 순서를 처리하는 RNN 구조가 아니기 때문에, 입력 데이터의 위치 정보를 추가적으로 제공.
  - 이 정보는 입력 임베딩에 더해지며, 사인과 코사인 함수를 사용해 계산.
<b>5. 피드포워드 신경망(Feedforward Neural Network)</b>
  - 각 Attention Block 뒤에는 독립적인 피드포워드 신경망이 포함돼, 비선형 변환과 학습 가능.
<b>6. 레이어 정규화(Layer Normalization)와 잔차 연결(Residual Connection)</b>
  - 모델 훈련을 안정화하고, 더 깊은 구조에서도 정보 손실을 방지.

### Transformer의 특징
1. 병렬 처리
  - 기존 RNN 기반 모델은 순차적으로 데이터를 처리했지만, Transformer는 병렬 처리가 가능해 학습 속도가 빠르다.
2. 스케일 확장 가능성
  - 더 큰 데이터와 더 큰 모델에서 성능이 우수하게 확장됨. GPT, BERT, T5 등의 대규모 모델의 기반 구조.
3. 다양한 응용 가능
  - 언어 번역, 문서 생성, 요약, 질문 답변, 코딩 지원 등 다양한 NLP 작업에 사용.